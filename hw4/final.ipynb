{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQKc1AptWkEFkFmmiAHR55"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UQgyIjJ8EwAN"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision \n",
        "import torchvision.transforms as transforms\n",
        "import time\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Preparing for Data\n",
        "print('==> Preparing data..')\n",
        "\n",
        "# Training Data augmentation\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "# Testing Data preparation\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yZ2wteHE_S9",
        "outputId": "4ced8fcb-5cb4-48bb-83fb-89c33de512be"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Preparing data..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class LeNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(3, 6, 5)  # 3 input channels, 6 output channels, 5x5 kernel\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # 2x2 max pooling with stride 2\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5) # 6 input channels, 16 output channels, 5x5 kernel\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120) # fully connected layer with 120 output units\n",
        "        self.fc2 = nn.Linear(120, 84) # fully connected layer with 84 output units\n",
        "        self.fc3 = nn.Linear(84, 10) # fully connected layer with 10 output units (for 10 classes)\n",
        "        self.dropout1 = nn.Dropout(p=0.5) \n",
        "        self.dropout2 = nn.Dropout(p=0.5)\n",
        "        self.dropout = True\n",
        "        self.batch_norm = False\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.pool(F.relu(self.conv1(x)))  # 1st conv layer, ReLU activation, max pooling\n",
        "        x = self.pool(F.relu(self.conv2(x)))  # 2nd conv layer, ReLU activation, max pooling\n",
        "        x = x.view(-1, 16 * 5 * 5)           # flatten output of conv layer\n",
        "        x = F.relu(self.fc1(x))              # 1st fully connected layer, ReLU activation\n",
        "        x = F.relu(self.fc2(x))              # 2nd fully connected layer, ReLU activation\n",
        "        if self.dropout:                     # add dropout layer if enabled\n",
        "            x = F.dropout(x, training=self.training)\n",
        "        x = self.fc3(x)                      # output layer\n",
        "\n",
        "        return x\n",
        "\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    count = 0\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = F.cross_entropy(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if batch_idx % 10 == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "def test( model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4GNhVDJeFC5K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    time0 = time.time()\n",
        "    # Training settings\n",
        "    batch_size = 128\n",
        "    epochs = 20\n",
        "    lr = 0.05\n",
        "    no_cuda = False\n",
        "    save_model = False\n",
        "    use_cuda = not no_cuda and torch.cuda.is_available()\n",
        "    torch.manual_seed(100)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    \n",
        "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    test_loader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False)\n",
        "\n",
        "    model = LeNet().to(device)\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train( model, device, train_loader, optimizer, epoch)\n",
        "        test( model, device, test_loader)\n",
        "\n",
        "    if (save_model):\n",
        "        torch.save(model.state_dict(),\"cifar_lenet.pt\")\n",
        "    time1 = time.time() \n",
        "    print ('Traning and Testing total excution time is: %s seconds ' % (time1-time0))   \n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4CZgyzka_zeC",
        "outputId": "54dc7c62-f722-4f1b-a020-6fd557223cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 69285508.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Train Epoch: 1 [0/50000 (0%)]\tLoss: 2.313206\n",
            "Train Epoch: 1 [1280/50000 (3%)]\tLoss: 2.290324\n",
            "Train Epoch: 1 [2560/50000 (5%)]\tLoss: 2.256552\n",
            "Train Epoch: 1 [3840/50000 (8%)]\tLoss: 2.170691\n",
            "Train Epoch: 1 [5120/50000 (10%)]\tLoss: 2.070918\n",
            "Train Epoch: 1 [6400/50000 (13%)]\tLoss: 2.066700\n",
            "Train Epoch: 1 [7680/50000 (15%)]\tLoss: 2.069912\n",
            "Train Epoch: 1 [8960/50000 (18%)]\tLoss: 2.015686\n",
            "Train Epoch: 1 [10240/50000 (20%)]\tLoss: 1.980385\n",
            "Train Epoch: 1 [11520/50000 (23%)]\tLoss: 1.915211\n",
            "Train Epoch: 1 [12800/50000 (26%)]\tLoss: 1.879259\n",
            "Train Epoch: 1 [14080/50000 (28%)]\tLoss: 2.036569\n",
            "Train Epoch: 1 [15360/50000 (31%)]\tLoss: 1.935795\n",
            "Train Epoch: 1 [16640/50000 (33%)]\tLoss: 1.901657\n",
            "Train Epoch: 1 [17920/50000 (36%)]\tLoss: 1.942562\n",
            "Train Epoch: 1 [19200/50000 (38%)]\tLoss: 2.069175\n",
            "Train Epoch: 1 [20480/50000 (41%)]\tLoss: 1.872715\n",
            "Train Epoch: 1 [21760/50000 (43%)]\tLoss: 1.930039\n",
            "Train Epoch: 1 [23040/50000 (46%)]\tLoss: 1.912094\n",
            "Train Epoch: 1 [24320/50000 (49%)]\tLoss: 1.940092\n",
            "Train Epoch: 1 [25600/50000 (51%)]\tLoss: 1.767082\n",
            "Train Epoch: 1 [26880/50000 (54%)]\tLoss: 1.730553\n",
            "Train Epoch: 1 [28160/50000 (56%)]\tLoss: 1.895015\n",
            "Train Epoch: 1 [29440/50000 (59%)]\tLoss: 1.990828\n",
            "Train Epoch: 1 [30720/50000 (61%)]\tLoss: 1.852807\n",
            "Train Epoch: 1 [32000/50000 (64%)]\tLoss: 1.756836\n",
            "Train Epoch: 1 [33280/50000 (66%)]\tLoss: 1.714359\n",
            "Train Epoch: 1 [34560/50000 (69%)]\tLoss: 1.832345\n",
            "Train Epoch: 1 [35840/50000 (72%)]\tLoss: 1.894242\n",
            "Train Epoch: 1 [37120/50000 (74%)]\tLoss: 1.777378\n",
            "Train Epoch: 1 [38400/50000 (77%)]\tLoss: 1.656628\n",
            "Train Epoch: 1 [39680/50000 (79%)]\tLoss: 1.847139\n",
            "Train Epoch: 1 [40960/50000 (82%)]\tLoss: 1.794557\n",
            "Train Epoch: 1 [42240/50000 (84%)]\tLoss: 1.741862\n",
            "Train Epoch: 1 [43520/50000 (87%)]\tLoss: 1.825006\n",
            "Train Epoch: 1 [44800/50000 (90%)]\tLoss: 1.694186\n",
            "Train Epoch: 1 [46080/50000 (92%)]\tLoss: 1.663858\n",
            "Train Epoch: 1 [47360/50000 (95%)]\tLoss: 1.724674\n",
            "Train Epoch: 1 [48640/50000 (97%)]\tLoss: 1.683722\n",
            "Train Epoch: 1 [31200/50000 (100%)]\tLoss: 1.600575\n",
            "\n",
            "Test set: Average loss: -2.2639, Accuracy: 4198/10000 (42%)\n",
            "\n",
            "Train Epoch: 2 [0/50000 (0%)]\tLoss: 1.732875\n",
            "Train Epoch: 2 [1280/50000 (3%)]\tLoss: 1.707335\n",
            "Train Epoch: 2 [2560/50000 (5%)]\tLoss: 1.707358\n",
            "Train Epoch: 2 [3840/50000 (8%)]\tLoss: 1.763121\n",
            "Train Epoch: 2 [5120/50000 (10%)]\tLoss: 1.867169\n",
            "Train Epoch: 2 [6400/50000 (13%)]\tLoss: 1.661756\n",
            "Train Epoch: 2 [7680/50000 (15%)]\tLoss: 1.846747\n",
            "Train Epoch: 2 [8960/50000 (18%)]\tLoss: 1.761580\n",
            "Train Epoch: 2 [10240/50000 (20%)]\tLoss: 1.729485\n",
            "Train Epoch: 2 [11520/50000 (23%)]\tLoss: 1.722001\n",
            "Train Epoch: 2 [12800/50000 (26%)]\tLoss: 1.771881\n",
            "Train Epoch: 2 [14080/50000 (28%)]\tLoss: 1.731376\n",
            "Train Epoch: 2 [15360/50000 (31%)]\tLoss: 1.651317\n",
            "Train Epoch: 2 [16640/50000 (33%)]\tLoss: 1.597468\n",
            "Train Epoch: 2 [17920/50000 (36%)]\tLoss: 1.529061\n",
            "Train Epoch: 2 [19200/50000 (38%)]\tLoss: 1.762899\n",
            "Train Epoch: 2 [20480/50000 (41%)]\tLoss: 1.898025\n",
            "Train Epoch: 2 [21760/50000 (43%)]\tLoss: 1.644540\n",
            "Train Epoch: 2 [23040/50000 (46%)]\tLoss: 1.847353\n",
            "Train Epoch: 2 [24320/50000 (49%)]\tLoss: 1.726674\n",
            "Train Epoch: 2 [25600/50000 (51%)]\tLoss: 1.802500\n",
            "Train Epoch: 2 [26880/50000 (54%)]\tLoss: 1.710604\n",
            "Train Epoch: 2 [28160/50000 (56%)]\tLoss: 1.881559\n",
            "Train Epoch: 2 [29440/50000 (59%)]\tLoss: 1.571107\n",
            "Train Epoch: 2 [30720/50000 (61%)]\tLoss: 1.666766\n",
            "Train Epoch: 2 [32000/50000 (64%)]\tLoss: 1.716980\n",
            "Train Epoch: 2 [33280/50000 (66%)]\tLoss: 1.676667\n",
            "Train Epoch: 2 [34560/50000 (69%)]\tLoss: 1.878404\n",
            "Train Epoch: 2 [35840/50000 (72%)]\tLoss: 1.619181\n",
            "Train Epoch: 2 [37120/50000 (74%)]\tLoss: 1.604108\n",
            "Train Epoch: 2 [38400/50000 (77%)]\tLoss: 1.482155\n",
            "Train Epoch: 2 [39680/50000 (79%)]\tLoss: 1.726894\n",
            "Train Epoch: 2 [40960/50000 (82%)]\tLoss: 1.670359\n",
            "Train Epoch: 2 [42240/50000 (84%)]\tLoss: 1.717603\n",
            "Train Epoch: 2 [43520/50000 (87%)]\tLoss: 1.543041\n",
            "Train Epoch: 2 [44800/50000 (90%)]\tLoss: 1.581555\n",
            "Train Epoch: 2 [46080/50000 (92%)]\tLoss: 1.590290\n",
            "Train Epoch: 2 [47360/50000 (95%)]\tLoss: 1.375660\n",
            "Train Epoch: 2 [48640/50000 (97%)]\tLoss: 1.693291\n",
            "Train Epoch: 2 [31200/50000 (100%)]\tLoss: 1.708709\n",
            "\n",
            "Test set: Average loss: -2.1717, Accuracy: 4338/10000 (43%)\n",
            "\n",
            "Train Epoch: 3 [0/50000 (0%)]\tLoss: 1.751963\n",
            "Train Epoch: 3 [1280/50000 (3%)]\tLoss: 1.726025\n",
            "Train Epoch: 3 [2560/50000 (5%)]\tLoss: 1.829265\n",
            "Train Epoch: 3 [3840/50000 (8%)]\tLoss: 1.622265\n",
            "Train Epoch: 3 [5120/50000 (10%)]\tLoss: 1.784285\n",
            "Train Epoch: 3 [6400/50000 (13%)]\tLoss: 1.596222\n",
            "Train Epoch: 3 [7680/50000 (15%)]\tLoss: 1.620724\n",
            "Train Epoch: 3 [8960/50000 (18%)]\tLoss: 1.838909\n",
            "Train Epoch: 3 [10240/50000 (20%)]\tLoss: 1.570976\n",
            "Train Epoch: 3 [11520/50000 (23%)]\tLoss: 1.601030\n",
            "Train Epoch: 3 [12800/50000 (26%)]\tLoss: 1.572441\n",
            "Train Epoch: 3 [14080/50000 (28%)]\tLoss: 1.663397\n",
            "Train Epoch: 3 [15360/50000 (31%)]\tLoss: 1.854679\n",
            "Train Epoch: 3 [16640/50000 (33%)]\tLoss: 1.641920\n",
            "Train Epoch: 3 [17920/50000 (36%)]\tLoss: 1.732838\n",
            "Train Epoch: 3 [19200/50000 (38%)]\tLoss: 1.701855\n",
            "Train Epoch: 3 [20480/50000 (41%)]\tLoss: 1.599496\n",
            "Train Epoch: 3 [21760/50000 (43%)]\tLoss: 1.529257\n",
            "Train Epoch: 3 [23040/50000 (46%)]\tLoss: 1.636622\n",
            "Train Epoch: 3 [24320/50000 (49%)]\tLoss: 1.583246\n",
            "Train Epoch: 3 [25600/50000 (51%)]\tLoss: 1.740833\n",
            "Train Epoch: 3 [26880/50000 (54%)]\tLoss: 1.606865\n",
            "Train Epoch: 3 [28160/50000 (56%)]\tLoss: 1.720138\n",
            "Train Epoch: 3 [29440/50000 (59%)]\tLoss: 1.642174\n",
            "Train Epoch: 3 [30720/50000 (61%)]\tLoss: 1.512522\n",
            "Train Epoch: 3 [32000/50000 (64%)]\tLoss: 1.496767\n",
            "Train Epoch: 3 [33280/50000 (66%)]\tLoss: 1.524591\n",
            "Train Epoch: 3 [34560/50000 (69%)]\tLoss: 1.636111\n",
            "Train Epoch: 3 [35840/50000 (72%)]\tLoss: 1.775777\n",
            "Train Epoch: 3 [37120/50000 (74%)]\tLoss: 1.695647\n",
            "Train Epoch: 3 [38400/50000 (77%)]\tLoss: 1.495134\n",
            "Train Epoch: 3 [39680/50000 (79%)]\tLoss: 1.407170\n",
            "Train Epoch: 3 [40960/50000 (82%)]\tLoss: 1.544014\n",
            "Train Epoch: 3 [42240/50000 (84%)]\tLoss: 1.540424\n",
            "Train Epoch: 3 [43520/50000 (87%)]\tLoss: 1.798407\n",
            "Train Epoch: 3 [44800/50000 (90%)]\tLoss: 1.719505\n",
            "Train Epoch: 3 [46080/50000 (92%)]\tLoss: 1.633610\n",
            "Train Epoch: 3 [47360/50000 (95%)]\tLoss: 1.730369\n",
            "Train Epoch: 3 [48640/50000 (97%)]\tLoss: 1.727974\n",
            "Train Epoch: 3 [31200/50000 (100%)]\tLoss: 1.616247\n",
            "\n",
            "Test set: Average loss: -2.2398, Accuracy: 4479/10000 (45%)\n",
            "\n",
            "Train Epoch: 4 [0/50000 (0%)]\tLoss: 1.550592\n",
            "Train Epoch: 4 [1280/50000 (3%)]\tLoss: 1.624053\n",
            "Train Epoch: 4 [2560/50000 (5%)]\tLoss: 1.619586\n",
            "Train Epoch: 4 [3840/50000 (8%)]\tLoss: 1.639238\n",
            "Train Epoch: 4 [5120/50000 (10%)]\tLoss: 1.539618\n",
            "Train Epoch: 4 [6400/50000 (13%)]\tLoss: 1.575206\n",
            "Train Epoch: 4 [7680/50000 (15%)]\tLoss: 1.735982\n",
            "Train Epoch: 4 [8960/50000 (18%)]\tLoss: 1.676021\n",
            "Train Epoch: 4 [10240/50000 (20%)]\tLoss: 1.548211\n",
            "Train Epoch: 4 [11520/50000 (23%)]\tLoss: 1.699653\n",
            "Train Epoch: 4 [12800/50000 (26%)]\tLoss: 1.603174\n",
            "Train Epoch: 4 [14080/50000 (28%)]\tLoss: 1.859672\n",
            "Train Epoch: 4 [15360/50000 (31%)]\tLoss: 1.675512\n",
            "Train Epoch: 4 [16640/50000 (33%)]\tLoss: 1.575074\n",
            "Train Epoch: 4 [17920/50000 (36%)]\tLoss: 1.513525\n",
            "Train Epoch: 4 [19200/50000 (38%)]\tLoss: 1.702335\n",
            "Train Epoch: 4 [20480/50000 (41%)]\tLoss: 1.592720\n",
            "Train Epoch: 4 [21760/50000 (43%)]\tLoss: 1.726055\n",
            "Train Epoch: 4 [23040/50000 (46%)]\tLoss: 1.641438\n",
            "Train Epoch: 4 [24320/50000 (49%)]\tLoss: 1.684381\n",
            "Train Epoch: 4 [25600/50000 (51%)]\tLoss: 1.835640\n",
            "Train Epoch: 4 [26880/50000 (54%)]\tLoss: 1.556757\n",
            "Train Epoch: 4 [28160/50000 (56%)]\tLoss: 1.334321\n",
            "Train Epoch: 4 [29440/50000 (59%)]\tLoss: 1.561746\n",
            "Train Epoch: 4 [30720/50000 (61%)]\tLoss: 1.603557\n",
            "Train Epoch: 4 [32000/50000 (64%)]\tLoss: 1.622710\n",
            "Train Epoch: 4 [33280/50000 (66%)]\tLoss: 1.624388\n",
            "Train Epoch: 4 [34560/50000 (69%)]\tLoss: 1.632043\n",
            "Train Epoch: 4 [35840/50000 (72%)]\tLoss: 1.518600\n",
            "Train Epoch: 4 [37120/50000 (74%)]\tLoss: 1.462903\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "04eYh91eOarq"
      }
    }
  ]
}
